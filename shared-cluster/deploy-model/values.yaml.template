# Model deployment configuration
model:
  # Model identifier - used for naming resources
  name: "<YOUR_MODEL_NAME>"
  # Display name for the model
  displayName: "<YOUR_MODEL_DISPLAY_NAME>"
  # Model source URI. Examples:
  #   - OCI:  oci://<YOUR_MODELCAR_MODEL_URI>
  #   - HTTP: https://<bucket-or-endpoint>/<path-to-model>
  #   - S3:   s3://<bucket>/<path-to-model>
  sourceUri: "oci://<YOUR_MODELCAR_MODEL_URI>"
  # Model format for KServe
  format: "vLLM"

# Namespace configuration
namespace:
  # Name of the namespace to deploy resources into
  name: "genai25-deployments"

# Serving runtime configuration
runtime:
  # Name of the serving runtime
  name: "model-inference-service"
  # Display name for the runtime
  displayName: "vLLM NVIDIA GPU ServingRuntime"
  # Runtime image
  image: "quay.io/modh/vllm@sha256:56aa86c6ed6ba6cc9557a8583ff9d4ee535193f6cda030bd1268064bc70120e3"
  # Extra command-line flags for the runtime container (optional)
  # extraArgs:
  #   - --chat-template=/app/data/template/template_gptoss.jinja
  #   - --max-model-len=464
  #   - --gpu_memory_utilization=0.95
  # Runtime annotations
  annotations:
    acceleratorName: "migrated-gpu"
    apiProtocol: "REST"
    hardwareProfileName: "migrated-gpu-mglzi-serving"
    recommendedAccelerators: '["nvidia.com/gpu"]'
    runtimeVersion: "v0.9.1.0"
    servingRuntimeScope: "global"
    templateDisplayName: "vLLM NVIDIA GPU ServingRuntime for KServe"
    templateName: "vllm-cuda-runtime"

# Inference service configuration
inference:
  # Name of the inference service
  name: "model-inference-service"
  # Display name for the inference service
  displayName: "Model Inference Service"
  # Autoscaling configuration
  scaling:
    minReplicas: 1
    maxReplicas: 1
  # Resource requirements
  resources:
    requests:
      cpu: "4"
      memory: "8Gi"
      gpu: "1"
    limits:
      cpu: "8"
      memory: "10Gi"
      gpu: "1"
  # GPU tolerations
  tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
  # Container registry secret (optional). Leave unset when using HTTPS/S3 model URIs.
connection:
  name: ""
