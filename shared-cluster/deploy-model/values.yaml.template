# Model deployment configuration
model:
  # Model identifier - used for naming resources
  name: "<YOUR_MODEL_NAME>"
  # Display name for the model
  displayName: "<YOUR_MODEL_DISPLAY_NAME>"
  # Model source URI in OCI format
  sourceUri: "oci://<YOUR_MODELCAR_MODEL_URI>"
  # Model format for KServe
  format: "vLLM"

# Namespace configuration
namespace:
  # Name of the namespace to deploy into
  name: "genai25-deployments"
  # Display name for the namespace
  displayName: "genai25-deployments"

# Serving runtime configuration
runtime:
  # Name of the serving runtime
  name: "model-inference-service"
  # Display name for the runtime
  displayName: "vLLM NVIDIA GPU ServingRuntime"
  # Runtime image
  image: "quay.io/modh/vllm@sha256:56aa86c6ed6ba6cc9557a8583ff9d4ee535193f6cda030bd1268064bc70120e3"
  # Runtime annotations
  annotations:
    acceleratorName: "migrated-gpu"
    apiProtocol: "REST"
    hardwareProfileName: "migrated-gpu-mglzi-serving"
    recommendedAccelerators: '["nvidia.com/gpu"]'
    runtimeVersion: "v0.9.1.0"
    servingRuntimeScope: "global"
    templateDisplayName: "vLLM NVIDIA GPU ServingRuntime for KServe"
    templateName: "vllm-cuda-runtime"

# Inference service configuration
inference:
  # Name of the inference service
  name: "model-inference-service"
  # Display name for the inference service
  displayName: "Model Inference Service"
  # Autoscaling configuration
  scaling:
    minReplicas: 1
    maxReplicas: 1
  # Resource requirements
  resources:
    requests:
      cpu: "4"
      memory: "8Gi"
      gpu: "1"
    limits:
      cpu: "8"
      memory: "10Gi"
      gpu: "1"
  # GPU tolerations
  tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
